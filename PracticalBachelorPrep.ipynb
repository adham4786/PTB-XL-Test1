{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHKu3mDLz5VA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMoiSDt90Xtr"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a6a4bed",
        "outputId": "e80b7cb2-4b5a-46f0-ae1b-245b24026a9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wfdb in c:\\users\\adrian\\vscode_projects\\bachelorprep\\ptb-xl-test1\\.venv\\lib\\site-packages (4.3.0)\n",
            "Requirement already satisfied: aiohttp>=3.10.11 in c:\\users\\adrian\\vscode_projects\\bachelorprep\\ptb-xl-test1\\.venv\\lib\\site-packages (from wfdb) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.10.0 in c:\\users\\adrian\\vscode_projects\\bachelorprep\\ptb-xl-test1\\.venv\\lib\\site-packages (from wfdb) (2025.9.0)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in c:\\users\\adrian\\vscode_projects\\bachelorprep\\ptb-xl-test1\\.venv\\lib\\site-packages (from wfdb) (3.10.7)\n",
            "Requirement already satisfied: numpy>=1.26.4 in c:\\users\\adrian\\vscode_projects\\bachelorprep\\ptb-xl-test1\\.venv\\lib\\site-packages (from wfdb) (2.3.4)\n",
            "Requirement already satisfied: pandas>=2.2.3 in c:\\users\\adrian\\vscode_projects\\bachelorprep\\ptb-xl-test1\\.venv\\lib\\site-packages (from wfdb) (2.3.3)\n",
            "Requirement already satisfied: requests>=2.8.1 in c:\\users\\adrian\\vscode_projects\\bachelorprep\\ptb-xl-test1\\.venv\\lib\\site-packages (from wfdb) (2.32.5)\n",
            "Requirement already satisfied: scipy>=1.13.0 in c:\\users\\adrian\\vscode_projects\\bachelorprep\\ptb-xl-test1\\.venv\\lib\\site-packages (from wfdb) (1.16.2)\n",
            "Requirement already satisfied: soundfile>=0.10.0 in c:\\users\\adrian\\vscode_projects\\bachelorprep\\ptb-xl-test1\\.venv\\lib\\site-packages (from wfdb) (0.13.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\adrian\\vscode_projects\\bachelorprep\\ptb-xl-test1\\.venv\\lib\\site-packages (from aiohttp>=3.10.11->wfdb) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\adrian\\vscode_projects\\bachelorprep\\ptb-xl-test1\\.venv\\lib\\site-packages (from aiohttp>=3.10.11->wfdb) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\adrian\\vscode_projects\\bachelorprep\\ptb-xl-test1\\.venv\\lib\\site-packages (from aiohttp>=3.10.11->wfdb) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\adrian\\vscode_projects\\bachelorprep\\ptb-xl-test1\\.venv\\lib\\site-packages (from aiohttp>=3.10.11->wfdb) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\adrian\\vscode_projects\\bachelorprep\\ptb-xl-test1\\.venv\\lib\\site-packages (from aiohttp>=3.10.11->wfdb) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\adrian\\vscode_projects\\bachelorprep\\ptb-xl-test1\\.venv\\lib\\site-packages (from aiohttp>=3.10.11->wfdb) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\adrian\\vscode_projects\\bachelorprep\\ptb-xl-test1\\.venv\\lib\\site-packages (from aiohttp>=3.10.11->wfdb) (1.22.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\adrian\\vscode_projects\\bachelorprep\\ptb-xl-test1\\.venv\\lib\\site-packages (from matplotlib>=3.2.2->wfdb) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\adrian\\vscode_projects\\bachelorprep\\ptb-xl-test1\\.venv\\lib\\site-packages (from matplotlib>=3.2.2->wfdb) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\adrian\\vscode_projects\\bachelorprep\\ptb-xl-test1\\.venv\\lib\\site-packages (from matplotlib>=3.2.2->wfdb) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\adrian\\vscode_projects\\bachelorprep\\ptb-xl-test1\\.venv\\lib\\site-packages (from matplotlib>=3.2.2->wfdb) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\adrian\\vscode_projects\\bachelorprep\\ptb-xl-test1\\.venv\\lib\\site-packages (from matplotlib>=3.2.2->wfdb) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in c:\\users\\adrian\\vscode_projects\\bachelorprep\\ptb-xl-test1\\.venv\\lib\\site-packages (from matplotlib>=3.2.2->wfdb) (12.0.0)\n",
            "Requirement already satisfied: pyparsing>=3 in c:\\users\\adrian\\vscode_projects\\bachelorprep\\ptb-xl-test1\\.venv\\lib\\site-packages (from matplotlib>=3.2.2->wfdb) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\adrian\\vscode_projects\\bachelorprep\\ptb-xl-test1\\.venv\\lib\\site-packages (from matplotlib>=3.2.2->wfdb) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\adrian\\vscode_projects\\bachelorprep\\ptb-xl-test1\\.venv\\lib\\site-packages (from pandas>=2.2.3->wfdb) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\adrian\\vscode_projects\\bachelorprep\\ptb-xl-test1\\.venv\\lib\\site-packages (from pandas>=2.2.3->wfdb) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\adrian\\vscode_projects\\bachelorprep\\ptb-xl-test1\\.venv\\lib\\site-packages (from requests>=2.8.1->wfdb) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\adrian\\vscode_projects\\bachelorprep\\ptb-xl-test1\\.venv\\lib\\site-packages (from requests>=2.8.1->wfdb) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\adrian\\vscode_projects\\bachelorprep\\ptb-xl-test1\\.venv\\lib\\site-packages (from requests>=2.8.1->wfdb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\adrian\\vscode_projects\\bachelorprep\\ptb-xl-test1\\.venv\\lib\\site-packages (from requests>=2.8.1->wfdb) (2025.10.5)\n",
            "Requirement already satisfied: cffi>=1.0 in c:\\users\\adrian\\vscode_projects\\bachelorprep\\ptb-xl-test1\\.venv\\lib\\site-packages (from soundfile>=0.10.0->wfdb) (2.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in c:\\users\\adrian\\vscode_projects\\bachelorprep\\ptb-xl-test1\\.venv\\lib\\site-packages (from aiosignal>=1.4.0->aiohttp>=3.10.11->wfdb) (4.15.0)\n",
            "Requirement already satisfied: pycparser in c:\\users\\adrian\\vscode_projects\\bachelorprep\\ptb-xl-test1\\.venv\\lib\\site-packages (from cffi>=1.0->soundfile>=0.10.0->wfdb) (2.23)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\adrian\\vscode_projects\\bachelorprep\\ptb-xl-test1\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->wfdb) (1.17.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install wfdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7a3d6520",
        "outputId": "702cfb1d-9d3f-4d8d-ce8b-ccde5e47558b"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'scp_codes'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 57\u001b[39m\n\u001b[32m     54\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m Y_filtered = Y[\u001b[43mY\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mscp_codes\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m.apply(contains_relevant_code)].copy()\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# Display the first few rows of the filtered metadata\u001b[39;00m\n\u001b[32m     61\u001b[39m display(Y_filtered.head())\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Adrian\\VSCode_Projects\\bachelorPrep\\PTB-XL-Test1\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Adrian\\VSCode_Projects\\bachelorPrep\\PTB-XL-Test1\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:417\u001b[39m, in \u001b[36mRangeIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    415\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[32m--> \u001b[39m\u001b[32m417\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[32m    418\u001b[39m \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
            "\u001b[31mKeyError\u001b[39m: 'scp_codes'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import wfdb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "import requests\n",
        "\n",
        "# Define data directory relative to the current working directory\n",
        "data_dir = './data'\n",
        "records_500hz_dir_relative = 'records500'\n",
        "records_500hz_dir_full = os.path.join(data_dir, records_500hz_dir_relative)\n",
        "\n",
        "# Get a list of all record names (without extensions) in the records500 directory\n",
        "# Assuming record names are in the format 'XXXXX_hr'\n",
        "record_names = [f.split('.')[0] for f in os.listdir(records_500hz_dir_full) if f.endswith('.hea')]\n",
        "record_names = sorted(list(set(record_names))) # Remove duplicates and sort\n",
        "\n",
        "# Function to load metadata from a .hea file\n",
        "def load_metadata_from_header(record_name):\n",
        "    try:\n",
        "        header = wfdb.rdheader(record_name, pn_dir=records_500hz_dir_full)\n",
        "        # Extract relevant information from the header\n",
        "        # This will depend on what information you need (e.g., sampling frequency, number of leads)\n",
        "        # For 'NORM' and 'MI' classification, we need the diagnostic codes.\n",
        "        # The diagnostic codes are usually in the comments of the header file.\n",
        "        # We'll need to parse the comments to find the 'scp_codes'.\n",
        "        scp_codes = {}\n",
        "        for comment in header.comments:\n",
        "            if comment.startswith('SCP'):\n",
        "                 # Assuming the format is 'SCP codes: {<code>: <value>}'\n",
        "                 try:\n",
        "                     scp_codes_str = comment.split('SCP codes: ')[1]\n",
        "                     scp_codes = ast.literal_eval(scp_codes_str)\n",
        "                 except:\n",
        "                    pass # Handle potential parsing errors\n",
        "\n",
        "        return {'record_name': record_name, 'scp_codes': scp_codes, 'sampling_frequency': header.fs, 'num_leads': header.n_sig}\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading header for {record_name}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Load metadata for all records\n",
        "metadata_list = [load_metadata_from_header(record_name) for record_name in record_names]\n",
        "\n",
        "# Convert the list of dictionaries to a pandas DataFrame\n",
        "Y = pd.DataFrame([m for m in metadata_list if m is not None])\n",
        "\n",
        "# Filter for 'NORM' and 'MI' classes\n",
        "relevant_scp_codes = ['NORM', 'MI']\n",
        "def contains_relevant_code(scp_codes_dict):\n",
        "    if isinstance(scp_codes_dict, dict):\n",
        "        for code in scp_codes_dict.keys():\n",
        "            if code in relevant_scp_codes:\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "Y_filtered = Y[Y[\"scp_codes\"].apply(contains_relevant_code)].copy()\n",
        "\n",
        "\n",
        "# Display the first few rows of the filtered metadata\n",
        "display(Y_filtered.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bbda296"
      },
      "source": [
        "### Subtask:\n",
        "Evaluate the trained model's performance on a separate validation set using appropriate metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "436cb35b"
      },
      "source": [
        "**Reasoning**:\n",
        "Evaluating the model on a validation set provides an estimate of its performance on unseen data and helps to assess how well it generalizes. We will use metrics like accuracy and potentially others relevant to classification tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fee2cf31",
        "outputId": "4bf40be2-076b-4c50-fd5f-5a15ec08b2da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on the validation set: 1.0000\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Initialize variables to track correct predictions and total predictions\n",
        "correct_predictions = 0\n",
        "total_predictions = 0\n",
        "\n",
        "# Disable gradient calculation for evaluation\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in val_dataloader:\n",
        "        # Move data to the appropriate device\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Get the predicted class\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        # Update counts\n",
        "        total_predictions += labels.size(0)\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = correct_predictions / total_predictions\n",
        "\n",
        "print(f\"Accuracy on the validation set: {accuracy:.4f}\")\n",
        "\n",
        "# Note: This evaluation is based on the training that was performed on dummy data\n",
        "# and was potentially interrupted. The results will not reflect the model's\n",
        "# performance on the actual ECG dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37c5c9e5"
      },
      "source": [
        "### Subtask:\n",
        "Implement the training loop for the 1D CNN model using PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd753ff4"
      },
      "source": [
        "**Reasoning**:\n",
        "The training loop is where the model learns from the data. It involves iterating over the training data in batches, calculating the loss, computing gradients, and updating the model's weights using an optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "ba476cad",
        "outputId": "59996af2-5d5e-461e-a6f4-999248656ce1"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-454379113.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# Backward pass and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    514\u001b[0m                             )\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    245\u001b[0m             )\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    248\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mmaybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdisabled_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_fallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    950\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[0m\n\u001b[1;32m    533\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0;31m# Lastly, switch back to complex view\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "# Instantiate the model, loss function, and optimizer\n",
        "model = ECGNet(num_classes=2) # Make sure ECGNet class is defined in a previous cell\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001) # You can adjust the learning rate\n",
        "\n",
        "# Set up device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10 # You can adjust the number of epochs\n",
        "for epoch in range(num_epochs):\n",
        "    model.train() # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(train_dataloader):\n",
        "        # Move data to the appropriate device\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Print epoch statistics\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_dataloader):.4f}\")\n",
        "\n",
        "print(\"Finished Training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "934248a9"
      },
      "source": [
        "### Subtask:\n",
        "Define the architecture of the 1D CNN model using PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc54c159"
      },
      "source": [
        "**Reasoning**:\n",
        "A 1D CNN is suitable for processing sequential data like ECG signals. The model will consist of convolutional layers to extract features, pooling layers to reduce dimensionality, and fully connected layers for classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4f9328b1"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch # Import torch to use transpose\n",
        "\n",
        "class ECGNet(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(ECGNet, self).__init__()\n",
        "        # Define the convolutional layers\n",
        "        self.conv1 = nn.Conv1d(in_channels=12, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
        "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2)\n",
        "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=5, stride=1, padding=2)\n",
        "        self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Define fully connected layers\n",
        "        # Calculate the output size of the convolutional layers to determine the input size of the first fully connected layer\n",
        "        # This requires knowing the input signal length and how the pooling layers affect it.\n",
        "        # For a signal length of 5000 and kernel_size=2, stride=2 for pooling:\n",
        "        # After pool1: (5000 - 2)/2 + 1 = 2499\n",
        "        # After pool2: (2499 - 2)/2 + 1 = 1249 (integer division might vary) -> let's double check or use a dummy forward pass\n",
        "        # After pool3: (1249 - 2)/2 + 1 = 624 (integer division might vary) -> let's double check or use a dummy forward pass\n",
        "\n",
        "        # A safer way is to calculate it dynamically or pass a dummy tensor through.\n",
        "        # Assuming input shape (batch_size, num_leads, signal_length) -> (batch_size, 12, 5000)\n",
        "        def _get_conv_output_size(length):\n",
        "            size = (length + 2 * 2 - 5) // 1 + 1 # conv1\n",
        "            size = (size - 2) // 2 + 1 # pool1\n",
        "            size = (size + 2 * 2 - 5) // 1 + 1 # conv2\n",
        "            size = (size - 2) // 2 + 1 # pool2\n",
        "            size = (size + 2 * 2 - 5) // 1 + 1 # conv3\n",
        "            size = (size - 2) // 2 + 1 # pool3\n",
        "            return size\n",
        "\n",
        "        fc_input_size = _get_conv_output_size(5000) * 128 # multiply by the number of output channels from the last conv layer\n",
        "\n",
        "        self.fc1 = nn.Linear(fc_input_size, 256)\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input shape: (batch_size, signal_length, num_leads) from DataLoader\n",
        "        # Transpose to (batch_size, num_leads, signal_length) for Conv1d\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool1(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool2(x)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.pool3(x)\n",
        "\n",
        "        # Flatten the output for the fully connected layers\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Example of creating the model\n",
        "# model = ECGNet(num_classes=2)\n",
        "# print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "141d6f27"
      },
      "source": [
        "### Subtask:\n",
        "Prepare the filtered ECG data and labels for use with PyTorch, creating custom `Dataset` and `DataLoader` classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c5484a8"
      },
      "source": [
        "**Reasoning**:\n",
        "To train a PyTorch model, the data needs to be organized into `Dataset` and `DataLoader` objects. The `Dataset` will handle loading individual samples (ECG signals and their corresponding labels), and the `DataLoader` will provide batches of data for training and validation, handling shuffling and parallel loading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aba1617",
        "outputId": "128411cf-1d24-4b17-d705-736e4a427271"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training samples: 7611\n",
            "Number of validation samples: 1903\n",
            "Number of training batches: 238\n",
            "Number of validation batches: 60\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np # Import numpy for creating dummy data\n",
        "\n",
        "class ECGDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Assuming data is a list/array of signals and labels is a pandas Series/numpy array\n",
        "        signal = self.data.iloc[idx] # Adjust indexing based on your data structure\n",
        "        label = self.labels.iloc[idx] # Adjust indexing based on your data structure\n",
        "\n",
        "        # Convert numpy arrays to PyTorch tensors\n",
        "        signal = torch.tensor(signal, dtype=torch.float32)\n",
        "        label = torch.tensor(label, dtype=torch.long) # Assuming labels are integers for classification\n",
        "\n",
        "        return signal, label\n",
        "\n",
        "# Assuming Y_filtered is your pandas DataFrame with 'scp_codes' column\n",
        "# And you have already filtered for 'NORM' and 'MI'\n",
        "\n",
        "# Map 'NORM' and 'MI' to numerical labels\n",
        "label_mapping = {'NORM': 0, 'MI': 1}\n",
        "Y_filtered['numeric_label'] = Y_filtered['scp_codes'].apply(lambda x: label_mapping.get(list(x.keys())[0], -1))\n",
        "\n",
        "# Remove rows with no relevant label (if any were missed)\n",
        "Y_filtered = Y_filtered[Y_filtered['numeric_label'] != -1].copy()\n",
        "\n",
        "# --- Temporary signal data for testing ---\n",
        "# Replace this section with the actual signal data loading,\n",
        "# when you download the full dataset.\n",
        "# Create dummy signal data: a list of numpy arrays.\n",
        "# The signal size (e.g., 5000 points) and number of leads (e.g., 12)\n",
        "# should match the real data.\n",
        "dummy_signal_length = 5000 # Assuming a signal length of 500 Hz\n",
        "dummy_num_leads = 12     # Assuming 12 leads\n",
        "Y_filtered['signal'] = [np.random.randn(dummy_signal_length, dummy_num_leads) for _ in range(len(Y_filtered))]\n",
        "# --- End of temporary signal data section ---\n",
        "\n",
        "\n",
        "# Separate signals and labels\n",
        "# Now 'signal' column exists with dummy data\n",
        "signals = Y_filtered['signal']\n",
        "labels = Y_filtered['numeric_label']\n",
        "\n",
        "# Split data into training and validation sets (simple split for now)\n",
        "# In a real project, you'd use train_test_split from sklearn\n",
        "train_size = int(0.8 * len(Y_filtered))\n",
        "val_size = len(Y_filtered) - train_size\n",
        "\n",
        "train_signals = signals[:train_size]\n",
        "train_labels = labels[:train_size]\n",
        "val_signals = signals[train_size:]\n",
        "val_labels = labels[train_size:]\n",
        "\n",
        "\n",
        "# Create Dataset and DataLoader instances\n",
        "train_dataset = ECGDataset(train_signals, train_labels)\n",
        "val_dataset = ECGDataset(val_signals, val_labels)\n",
        "\n",
        "batch_size = 32 # You can adjust this\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Number of training samples: {len(train_dataset)}\")\n",
        "print(f\"Number of validation samples: {len(val_dataset)}\")\n",
        "print(f\"Number of training batches: {len(train_dataloader)}\")\n",
        "print(f\"Number of validation batches: {len(val_dataloader)}\")\n",
        "\n",
        "# Example of accessing a batch\n",
        "# train_features, train_labels = next(iter(train_dataloader))\n",
        "# print(f\"Feature batch shape: {train_features.size()}\")\n",
        "# print(f\"Labels batch shape: {train_labels.size()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07064e8c"
      },
      "source": [
        "Now that the data is loaded and filtered, we need to preprocess the ECG signals. This involves:\n",
        "\n",
        "- Extracting the signal data for the filtered records.\n",
        "- Resampling the signals to a consistent length if necessary (although for this task at 500Hz, most signals might already be at a consistent length or close enough for a 1D CNN).\n",
        "- Normalizing the signal data.\n",
        "- Splitting the data into training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "f2f55e29",
        "outputId": "2f664f9b-3363-4f44-fbb3-4b0e1296993c"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'Y_filtered' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1990756601.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Apply the loading function to the filtered metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mY_filtered\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'signal'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_filtered\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'record_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_signal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Remove rows where signal loading failed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Y_filtered' is not defined"
          ]
        }
      ],
      "source": [
        "# Extract signal data for the filtered records\n",
        "# Assuming the signal files are organized by record_path in the metadata\n",
        "def load_signal(record_path):\n",
        "    # Correct the file path to point to the 500Hz records\n",
        "    file_path = os.path.join(data_dir, record_path).replace('records100', 'records500')\n",
        "    try:\n",
        "        # rdsamp returns a tuple: (signals, metadata)\n",
        "        signals, meta = wfdb.rdsamp(file_path)\n",
        "        return signals\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading signal {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Apply the loading function to the filtered metadata\n",
        "Y_filtered['signal'] = Y_filtered['record_path'].apply(load_signal)\n",
        "\n",
        "# Remove rows where signal loading failed\n",
        "Y_filtered.dropna(subset=['signal'], inplace=True)\n",
        "\n",
        "# Further preprocessing steps (normalization, consistent length, train/test split) would go here\n",
        "# For now, let's just confirm the data structure\n",
        "print(f\"Number of filtered records with loaded signals: {len(Y_filtered)}\")\n",
        "print(f\"Example signal shape: {Y_filtered['signal'].iloc[0].shape if len(Y_filtered) > 0 else 'No signals loaded'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40ba7f98"
      },
      "source": [
        "# Task\n",
        "Build and train a 1D CNN using PyTorch to classify ECG signals from the PTB-XL dataset into 'Normal ECG' and 'Myocardial Infarction' classes, using a sampling rate of 500Hz."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6161a370"
      },
      "source": [
        "## Load and preprocess data\n",
        "\n",
        "### Subtask:\n",
        "Load the PTB-XL dataset, focusing on the specified classes ('NORM' and 'MI'). Preprocess the ECG signals to a consistent format suitable for the CNN model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7d2c1b1"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the metadata and signal data from the specified file paths and filter the metadata to include only records with diagnostic classes 'NORM' and 'MI'.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
